{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before vectorizing, I have to pre-process the `text` column:\n",
    "\n",
    "- Converting all letters to lower case.\n",
    "- Turning the tweets into tokens. Tokens are words separated by spaces in a text.\n",
    "- Eliminating unwanted characters, such as punctuation marks, special characters, white spaces etc.\n",
    "- Remove stop words, defined by the nltk library.\n",
    "- Apply lemmatization, and returns a word to its base or dictionary form. Example: Better -> Good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download relevant packages used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the preprocessed data from the previous step and randomly shuffle the dataset. This is because in the next step, we have to split the dataset into chunks and partially fit them into the model. Since the dataframe is arranged in a way where the first half entries are associated with positive sentiments and the other half associated with negative sentiments, we do not want the chunks to only contain tweet entries with a single sentiment (containing only -1 or 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_train = pd.read_csv('data/preprocess_data.csv')\n",
    "preprocess_train = preprocess_train.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "preprocess_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our predictor and target columns\n",
    "X = preprocess_train.drop(['sentiment'],axis=1)\n",
    "y = preprocess_train[['sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for Incremental Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better model evaluation, we will be splitting the whole dataset into train and validation set before vectorization. If not, we are carrying information from the train set over to the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I can vectorize the preprocessed `text` column:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the dataframe into predictor and target columns, I will be vectorizing the `text` column, with the following conditions set:\n",
    "- max_feature: only the top 10,000 features order by term frequency across the corpus, are considered in the model.\n",
    "- min_df and max_df: Ignores terms that appear in less than 0.1% in the documents and terms that appear in more than 50% of the documents.\n",
    "- stop_words: Pass through a list of stop words, containing frequently appearing terms like 'we','have','and', then ignoring them.\n",
    "- token_patterns: Ignoring terms that would be token that has one or more numbers in it.\n",
    "- strip_accents: Remove accents and perform other character normalization on characters that have an direct ASCII mapping https://www.ascii-code.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "# Vectorizing the text column\n",
    "X_text = X_train['text'].astype(str)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000, \n",
    "                                 min_df=0.0001, max_df=0.5, \n",
    "                                 stop_words=stopwords.words('english'),\n",
    "                                 token_pattern=r'\\b[^\\d\\W]+\\b',\n",
    "                                 strip_accents = \"ascii\")\n",
    "X_text = vectorizer.fit_transform(X_text)\n",
    "\n",
    "# Transforming the sparse matrix into dataframe\n",
    "X_text = pd.DataFrame(columns=vectorizer.get_feature_names(),data=X_text.toarray())\n",
    "X_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving numerical features from the preprocessed dataframe\n",
    "X_num = X_train.drop(['text'],axis=1)\n",
    "X_num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocabulary of the vectorizer, for transforming the twitter data\n",
    "import pickle\n",
    "pickle.dump(vectorizer.vocabulary_,open(\"pickle/feature.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I will merge all of the vectorized columns and the numerical columns in the train set, but since the dataframe is too big for concatenation, I will try to concatenate it by splitting the entire train data into 12 chunks, with 10,000 entries in each chunk, and then save them for modelling purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_num split\n",
    "n = 100000  #chunk row size\n",
    "list_num = [X_num[i:i+n] for i in range(0,X_num.shape[0],n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_text split\n",
    "n = 100000  #chunk row size\n",
    "list_text = [X_text[i:i+n] for i in range(0,X_text.shape[0],n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9,13):\n",
    "    X_train = pd.concat([list_num[i],list_text[i]],axis=1).astype('int64')\n",
    "    X_train.to_csv(f'/Users/JennaFu/Desktop/DataScience/BrainStation/Capstone/data/X_{i}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100000\n",
    "for i in range(0, len(y_train), batch_size):\n",
    "    y_train = y_train[i:i+batch_size]\n",
    "    y_train.to_csv(f'/Users/JennaFu/Desktop/DataScience/BrainStation/Capstone/data/y_{int(i/batch_size)}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the train data by chunks, I will be doing the some with the test data. Before splitting them into chunks, I will have to vectorize the test dataset as well using the vectorizer we have created with the train data, allowing the train and test datasets to have homogeneous features. This is because by transforming the validation data with the vectorizer, only the features in the train data will remain in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the validation set based on the vectorizer\n",
    "X_test_text = X_test['text'].values.astype('U')\n",
    "X_test_text = vectorizer.transform(X_val_text)\n",
    "X_test_text = pd.DataFrame(columns=vectorizer.get_feature_names(),data=X_test_text.toarray())\n",
    "\n",
    "# Retrieve numerical features in train and validation sets\n",
    "X_train_num = X_train.drop(['text'],axis=1)\n",
    "X_test_num = X_test.drop(['text'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I will be concatenating the text and numerical features, and saving the test data for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.concat([X_test_num,X_test_text],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(f'/Users/JennaFu/Desktop/DataScience/BrainStation/Capstone/data/X_test.csv')\n",
    "y_test.to_csv(f'/Users/JennaFu/Desktop/DataScience/BrainStation/Capstone/data/y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for Reduced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset our predictor and target columns\n",
    "X = preprocess_train.drop(['sentiment'],axis=1)\n",
    "y = preprocess_train[['sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than using incremental learning to train the model on the entire dataset (1.6 million entries), I also want to compare the results when the model is trained on a smaller subset of the entire dataset. I have choosen a sample of 10,000 entries from the dataset. Then using the same process as above, I have splitted the dataset into train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample datset X and y\n",
    "X = X.sample(n=100000, random_state=7).reset_index(drop=True)\n",
    "y = y.sample(n=100000, random_state=7).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same conditions, I have vectorized the preprocessed text column of the subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "# Vectorizing the text column\n",
    "X_train_text = X_train['text'].astype(str)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000, \n",
    "                                 min_df=0.0001, max_df=0.5, \n",
    "                                 stop_words=stopwords.words('english'),\n",
    "                                 token_pattern=r'\\b[^\\d\\W]+\\b',\n",
    "                                 strip_accents = \"ascii\")\n",
    "X_train_text = vectorizer.fit_transform(X_train_text)\n",
    "\n",
    "# Transforming the sparse matrix into dataframe\n",
    "X_train_text = pd.DataFrame(columns=vectorizer.get_feature_names(),data=X_train_text.toarray())\n",
    "X_train_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve numerical features from train set\n",
    "X_train_num = X_train.drop(['text'],axis=1)\n",
    "X_train_num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocabulary of the vectorizer, for transforming the twitter data\n",
    "import pickle\n",
    "pickle.dump(vectorizer.vocabulary_,open(\"pickle/reduced_feature.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be transforming the test data using the vectorizer we have created with the train data of the reduced dataset, agian to homogeneize the features of the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test set based on the vectorizer\n",
    "X_test_text = X_test['text'].values.astype('U')\n",
    "X_test_text = vectorizer.transform(X_test_text)\n",
    "X_test_text = pd.DataFrame(columns=vectorizer.get_feature_names(),data=X_test_text.toarray())\n",
    "X_test_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve numerical features from test set\n",
    "X_test_num = X_test.drop(['text'],axis=1)\n",
    "X_test_num.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we could merge the text and numerical features of the reduced train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train_num,X_train_text],axis=1)\n",
    "X_test = pd.concat([X_test_num,X_test_text],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(f'/Users/JennaFu/Desktop/DataScience/BrainStation/Capstone/data/reduced_model/X_train_reduced.csv')\n",
    "y_train.to_csv(f'/Users/JennaFu/Desktop/DataScience/BrainStation/Capstone/data/reduced_model/y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(f'/Users/JennaFu/Desktop/DataScience/BrainStation/Capstone/data/reduced_model/X_test_reduced.csv')\n",
    "y_test.to_csv(f'/Users/JennaFu/Desktop/DataScience/BrainStation/Capstone/data/reduced_model/y_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter",
   "language": "python",
   "name": "twitter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
