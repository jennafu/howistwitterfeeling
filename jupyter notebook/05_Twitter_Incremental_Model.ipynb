{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with Incremental Learning\n",
    "\n",
    "From the previous notebook, we have already found that the appropriate model and its respective hyperparameters. Now, I would apply the same model and parameters on this model, but instead with incremental learning. Incremental learning techniques are often used where data is processed in parts(subsets of the data are considered at any given point in time) and the result is then combined to save memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental Learning with SVM Explaination\n",
    "\n",
    "Given l data points {(x1,y1),(x2,y2),……..,(xl,yl)}, the decision function of SVMs is as follows:\n",
    "<img src=\"image/equation.png\" width=\"300\"/>\n",
    "Often, only a small fraction of alpha(i) coefficients are non-zero. Due to this, the corresponding xi entries and yi output labels fully define the decision function. These three are then preserved for use in the classification process. All the remaining training does not contribute in any way and is regarded as redundant. xi entries are the support vectors here.\n",
    "Since only a small number of data points end up as support vectors, the support vector algorithm is able to summarize the data space in a very concise manner. \n",
    "\n",
    "This is how incremental training works: \n",
    "1. Feed a subset of the data into the model.\n",
    "2. Preserve only the support vectors.\n",
    "3. Add them to the next subset.\n",
    "\n",
    "Reference: https://medium.com/computers-papers-and-everything/incremental-learning-with-support-vector-machines-e838cd2d7691"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers used\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental Learning with SVM Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the test sets\n",
    "X_test = pd.read_csv(f'data/incremental_model/X_test.csv',index_col=0)\n",
    "y_test = pd.read_csv(f'data/incremental_model/y_test.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the train and test sets\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Instantiate and fit to the train set\n",
    "SGDClass = SGDClassifier(loss='hinge',class_weight={-1: 1, 1: 5},random_state=7)\n",
    "\n",
    "# Run a loop to feed sub datasets into the model\n",
    "for i in range(0,13):\n",
    "    X = pd.read_csv(f'data/incremental_model/X_train/X_{i}.csv',index_col=0)\n",
    "    y = pd.read_csv(f'data/incremental_model/y_train/y_{i}.csv',index_col=0)\n",
    "        \n",
    "    # Partial fit the data to scaler\n",
    "    scaler.partial_fit(X)\n",
    "    scaler.transform(X)\n",
    "    \n",
    "    # Partial fit the data\n",
    "    SGDClass.partial_fit(X,y,classes=np.unique(y))\n",
    "    \n",
    "    # Progress\n",
    "    print(f\"Fitted sub-dataset {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SGDClass.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After partially fitting the entire dataset, we receive a test accuracy of only 50.09%, which is significantly lower than that of the model using the reduced dataset. Hence for now, it appears that the best option would be for us to go with the logistic model fitted with the reduced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model with Incremental Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'pickle/sgd_incremental_model.sav'\n",
    "# save the model to disk\n",
    "pickle.dump(SGDClass, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter",
   "language": "python",
   "name": "twitter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
