{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before vectorizing, I have to pre-process the `text` column:\n",
    "\n",
    "- Converting all letters to lower case.\n",
    "- Turning the tweets into tokens. Tokens are words separated by spaces in a text.\n",
    "- Eliminating unwanted characters, such as punctuation marks, special characters, white spaces etc.\n",
    "- Remove stop words, defined by the nltk library.\n",
    "- Apply lemmatization, and returns a word to its base or dictionary form. Example: Better -> Good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download relevant packages used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the preprocessed data from the previous step and randomly shuffle the dataset. This is because in the next step, we have to split the dataset into chunks and partially fit them into the model. Since the dataframe is arranged in a way where the first half entries are associated with positive sentiments and the other half associated with negative sentiments, we do not want the chunks to only contain tweet entries with a single sentiment (containing only -1 or 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Running heat recommended Although falling gett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>need dye hair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>whenever get bored ill alot since school need</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>bye homework</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>taylor swiftkeith urban boston MA 80 sure im g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0  Running heat recommended Although falling gett...\n",
       "1          0                                      need dye hair\n",
       "2          1      whenever get bored ill alot since school need\n",
       "3          0                                       bye homework\n",
       "4          1  taylor swiftkeith urban boston MA 80 sure im g..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_train = pd.read_csv('data/preprocess_data.csv')\n",
    "preprocess_train = preprocess_train.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "preprocess_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    797022\n",
       "0    795809\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_train['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our predictor and target columns\n",
    "X = preprocess_train.drop(['sentiment'],axis=1)\n",
    "y = preprocess_train[['sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for Incremental Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better model evaluation, we will be splitting the whole dataset into train and validation set before vectorization. If not, we are carrying information from the train set over to the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I can vectorize the preprocessed `text` column:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the dataframe into predictor and target columns, I will be vectorizing the `text` column, with the following conditions set:\n",
    "- max_feature: only the top 10,000 features order by term frequency across the corpus, are considered in the model.\n",
    "- min_df and max_df: Ignores terms that appear in less than 0.1% in the documents and terms that appear in more than 50% of the documents.\n",
    "- stop_words: Pass through a list of stop words, containing frequently appearing terms like 'we','have','and', then ignoring them.\n",
    "- token_patterns: Ignoring terms that would be token that has one or more numbers in it.\n",
    "- strip_accents: Remove accents and perform other character normalization on characters that have an direct ASCII mapping https://www.ascii-code.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1274264, 5598)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "# Vectorizing the text column\n",
    "X_text = X_train['text'].astype(str)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000, \n",
    "                                 min_df=0.0001, max_df=0.2, \n",
    "                                 stop_words=stopwords.words('english'),\n",
    "                                 token_pattern=r'\\b[^\\d\\W]+\\b',\n",
    "                                 strip_accents = \"ascii\")\n",
    "X_text = vectorizer.fit_transform(X_text)\n",
    "\n",
    "# Transforming the sparse matrix into dataframe\n",
    "X_text = pd.DataFrame(columns=vectorizer.get_feature_names(),data=X_text.toarray())\n",
    "X_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aah</th>\n",
       "      <th>aaron</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abc</th>\n",
       "      <th>ability</th>\n",
       "      <th>abit</th>\n",
       "      <th>able</th>\n",
       "      <th>...</th>\n",
       "      <th>yup</th>\n",
       "      <th>z</th>\n",
       "      <th>zac</th>\n",
       "      <th>zach</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zero</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274259</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274260</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274261</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274262</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274263</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1274264 rows Ã— 5598 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          aa  aaa  aaah  aah  aaron  abandoned  abc  ability  abit  able  ...  \\\n",
       "0        0.0  0.0   0.0  0.0    0.0        0.0  0.0      0.0   0.0   0.0  ...   \n",
       "1        0.0  0.0   0.0  0.0    0.0        0.0  0.0      0.0   0.0   0.0  ...   \n",
       "2        0.0  0.0   0.0  0.0    0.0        0.0  0.0      0.0   0.0   0.0  ...   \n",
       "3        0.0  0.0   0.0  0.0    0.0        0.0  0.0      0.0   0.0   0.0  ...   \n",
       "4        0.0  0.0   0.0  0.0    0.0        0.0  0.0      0.0   0.0   0.0  ...   \n",
       "...      ...  ...   ...  ...    ...        ...  ...      ...   ...   ...  ...   \n",
       "1274259  0.0  0.0   0.0  0.0    0.0        0.0  0.0      0.0   0.0   0.0  ...   \n",
       "1274260  0.0  0.0   0.0  0.0    0.0        0.0  0.0      0.0   0.0   0.0  ...   \n",
       "1274261  0.0  0.0   0.0  0.0    0.0        0.0  0.0      0.0   0.0   0.0  ...   \n",
       "1274262  0.0  0.0   0.0  0.0    0.0        0.0  0.0      0.0   0.0   0.0  ...   \n",
       "1274263  0.0  0.0   0.0  0.0    0.0        0.0  0.0      0.0   0.0   0.0  ...   \n",
       "\n",
       "         yup    z  zac  zach  zealand  zero  zombie  zombies  zone  zoo  \n",
       "0        0.0  0.0  0.0   0.0      0.0   0.0     0.0      0.0   0.0  0.0  \n",
       "1        0.0  0.0  0.0   0.0      0.0   0.0     0.0      0.0   0.0  0.0  \n",
       "2        0.0  0.0  0.0   0.0      0.0   0.0     0.0      0.0   0.0  0.0  \n",
       "3        0.0  0.0  0.0   0.0      0.0   0.0     0.0      0.0   0.0  0.0  \n",
       "4        0.0  0.0  0.0   0.0      0.0   0.0     0.0      0.0   0.0  0.0  \n",
       "...      ...  ...  ...   ...      ...   ...     ...      ...   ...  ...  \n",
       "1274259  0.0  0.0  0.0   0.0      0.0   0.0     0.0      0.0   0.0  0.0  \n",
       "1274260  0.0  0.0  0.0   0.0      0.0   0.0     0.0      0.0   0.0  0.0  \n",
       "1274261  0.0  0.0  0.0   0.0      0.0   0.0     0.0      0.0   0.0  0.0  \n",
       "1274262  0.0  0.0  0.0   0.0      0.0   0.0     0.0      0.0   0.0  0.0  \n",
       "1274263  0.0  0.0  0.0   0.0      0.0   0.0     0.0      0.0   0.0  0.0  \n",
       "\n",
       "[1274264 rows x 5598 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1277304, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieving numerical features from the preprocessed dataframe\n",
    "X_num = X_train.drop(['text'],axis=1)\n",
    "X_num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocabulary of the vectorizer, for transforming the twitter data\n",
    "import pickle\n",
    "pickle.dump(vectorizer.vocabulary_,open(\"new_pickle/feature.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I will merge all of the vectorized columns and the numerical columns in the train set, but since the dataframe is too big for concatenation, I will try to concatenate it by splitting the entire train data into 12 chunks, with 10,000 entries in each chunk, and then save them for modelling purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_num split\n",
    "n = 100000  #chunk row size\n",
    "list_num = [X_num[i:i+n] for i in range(0,X_num.shape[0],n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_text split\n",
    "n = 100000  #chunk row size\n",
    "list_text = [X_text[i:i+n] for i in range(0,X_text.shape[0],n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(9,13):\n",
    "#    X_train = pd.concat([list_num[i],list_text[i]],axis=1).astype('int64')\n",
    "#   X_train.to_csv(f'/Users/JennaFu/Desktop/DataScience/BrainStation/Capstone/data/X_{i}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 100000\n",
    "#for i in range(0, len(y_train), batch_size):\n",
    "#    y_train = y_train[i:i+batch_size]\n",
    "#    y_train.to_csv(f'/Users/JennaFu/Desktop/DataScience/BrainStation/Capstone/data/y_{int(i/batch_size)}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the train data by chunks, I will be doing the some with the test data. Before splitting them into chunks, I will have to vectorize the test dataset as well using the vectorizer we have created with the train data, allowing the train and test datasets to have homogeneous features. This is because by transforming the validation data with the vectorizer, only the features in the train data will remain in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the validation set based on the vectorizer\n",
    "#X_test_text = X_test['text'].values.astype('U')\n",
    "#X_test_text = vectorizer.transform(X_val_text)\n",
    "#X_test_text = pd.DataFrame(columns=vectorizer.get_feature_names(),data=X_test_text.toarray())\n",
    "\n",
    "# Retrieve numerical features in train and validation sets\n",
    "#X_train_num = X_train.drop(['text'],axis=1)\n",
    "#X_test_num = X_test.drop(['text'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I will be concatenating the text and numerical features, and saving the test data for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test = pd.concat([X_test_num,X_test_text],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test.to_csv(f'/Users/JennaFu/Desktop/DataScience/BrainStation/Capstone/data/X_test.csv')\n",
    "#y_test.to_csv(f'/Users/JennaFu/Desktop/DataScience/BrainStation/Capstone/data/y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for Reduced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset our predictor and target columns\n",
    "X = preprocess_train.drop(['sentiment'],axis=1)\n",
    "y = preprocess_train[['sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than using incremental learning to train the model on the entire dataset (1.6 million entries), I also want to compare the results when the model is trained on a smaller subset of the entire dataset. I have choosen a sample of 10,000 entries from the dataset. Then using the same process as above, I have splitted the dataset into train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample datset X and y\n",
    "X = X.sample(n=600000, random_state=7).reset_index(drop=True)\n",
    "y = y.sample(n=600000, random_state=7).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same conditions, I have vectorized the preprocessed text column of the subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480000, 8808)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "# Vectorizing the text column\n",
    "X_train_text = X_train['text'].astype(str)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000,\n",
    "                             ngram_range=(1,2),\n",
    "                                 min_df=0.0001, max_df=0.5, \n",
    "                                 stop_words=stopwords.words('english'),\n",
    "                                 token_pattern=r'\\b[^\\d\\W]+\\b',\n",
    "                                 strip_accents = \"ascii\")\n",
    "X_train_text = vectorizer.fit_transform(X_train_text)\n",
    "\n",
    "# Transforming the sparse matrix into dataframe\n",
    "X_train_text = pd.DataFrame(columns=vectorizer.get_feature_names(),data=X_train_text.toarray())\n",
    "X_train_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocabulary of the vectorizer, for transforming the twitter data\n",
    "import pickle\n",
    "pickle.dump(vectorizer.vocabulary_,open(\"pickle/reduced_new_feature.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be transforming the test data using the vectorizer we have created with the train data of the reduced dataset, agian to homogeneize the features of the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 8808)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the test set based on the vectorizer\n",
    "X_test_text = X_test['text'].values.astype('U')\n",
    "X_test_text = vectorizer.transform(X_test_text)\n",
    "X_test_text = pd.DataFrame(columns=vectorizer.get_feature_names(),data=X_test_text.toarray())\n",
    "X_test_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with Reduced Dataset (Without Numerical Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "# Classifiers used\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test_text\n",
    "X_train = X_train_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Fit the data to scaler\n",
    "scaler = RobustScaler()\n",
    "scaler.fit_transform(X_train)\n",
    "scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "0.802925\n",
      "0.76145\n",
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n",
      "0.802925\n",
      "0.76145\n",
      "RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
      "             with_scaling=True)\n",
      "0.802925\n",
      "0.76145\n"
     ]
    }
   ],
   "source": [
    "# Scale the train and test sets, with three types of scalers\n",
    "for scaler in [StandardScaler(), MinMaxScaler(), RobustScaler()]:\n",
    "    \n",
    "    # Fit the data to scaler\n",
    "    scaler.fit_transform(X_train)\n",
    "    scaler.transform(X_test)\n",
    "    \n",
    "    # Instantiate and fit to the train set\n",
    "    Logistic = LogisticRegression()\n",
    "    \n",
    "    # Fit the data\n",
    "    Logistic.fit(X_train,y_train)\n",
    "    \n",
    "    # Score the model\n",
    "    print(scaler)\n",
    "    print(Logistic.score(X_train,y_train))\n",
    "    print(Logistic.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "0.8159125\n",
      "0.75245\n",
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n",
      "0.8159125\n",
      "0.75245\n",
      "RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
      "             with_scaling=True)\n",
      "0.8159125\n",
      "0.75245\n"
     ]
    }
   ],
   "source": [
    "# Scale the train and test sets, with three types of scalers\n",
    "for scaler in [StandardScaler(), MinMaxScaler(), RobustScaler()]:\n",
    "    \n",
    "    # Fit the data to scaler\n",
    "    scaler.fit_transform(X_train)\n",
    "    scaler.transform(X_test)\n",
    "    \n",
    "    # Instantiate and fit to the train set\n",
    "    SVC = LinearSVC()\n",
    "    \n",
    "    # Fit the data\n",
    "    SVC.fit(X_train,y_train)\n",
    "    \n",
    "    # Score the model\n",
    "    print(scaler)\n",
    "    print(SVC.score(X_train,y_train))\n",
    "    print(SVC.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_comb = []\n",
    "svc_counter = []\n",
    "svc_score = []\n",
    "for i in [0.0001,0.001,0.01,0.1,1,10]:\n",
    "    for j in range(2,6):\n",
    "        clf = LinearSVC(C = i ,class_weight={0: 1, 1: j})\n",
    "        clf = clf.fit(X_train, y_train)\n",
    "        # Score the model\n",
    "        svc_comb.append([i,j])\n",
    "        svc_counter.append(Counter(clf.predict(X_test)))\n",
    "        svc_score.append(clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the data to scaler\n",
    "scaler = RobustScaler()\n",
    "scaler.fit_transform(X_train)\n",
    "scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78615625\n",
      "0.7734166666666666\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "clf = LinearSVC(C = 10)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# Score the model\n",
    "print(clf.score(X_train,y_train))\n",
    "print(clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'pickle/svc_new_model.sav'\n",
    "# save the model to disk\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44686, 15507],\n",
       "       [11683, 48124]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = clf.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter",
   "language": "python",
   "name": "twitter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
