{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Data Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import os\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "from matplotlib.dates import DateFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will have to gain access to the Twitter API, using my keys and secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your keys\n",
    "consumer_key = '2YCaHB1rnU7I7U8BuDJVqPGP2'\n",
    "consumer_secret = 'UJR0oFVc6JzaoWC6J2K2n3cMEfdAZS6nhJtHTGeHBnehrFVPZw'\n",
    "access_token = '1280193789756309511-b6F7ZCckvK3crRh7EzhfKk0sIJBYXQ'\n",
    "access_token_secret = '1I3YLccFnFoGzP0ekSWGLgXdUVBFHyVhnmvOs2ZQWX1XX'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, from the Tweepy API, I'm interested to find out what is the top 10 trending topics for Canada currently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import API\n",
    "WOEID = 2972 # WOEID for Canada\n",
    "# Returns the top 50 trending topics worldwide\n",
    "trends1 = api.trends_place(id = WOEID)\n",
    "# printing and listing the information\n",
    "print(\"The top 10 trends for the location are :\") \n",
    "topics = []\n",
    "for value in trends1: \n",
    "    for trend in value['trends'][0:10]:\n",
    "        topics.append(trend['name'].replace('#',''))\n",
    "        print(trend['name'].replace('#',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Topic-Specific Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will have to specify the period of time I want to retrieve my tweets from, in this case, I have specified the API to return tweets from the last 7 days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, date\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "start_date = (date.today() + timedelta(days=-6))\n",
    "end_date = (date.today() + timedelta(days=1))\n",
    "date_list = []\n",
    "for single_date in daterange(start_date, end_date):\n",
    "    date_list.append(single_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, using the `tweepy.Cursor` method, I was able to pull the most 1000 recent tweets from the last 7 days, with specific conditions like topics (text_query), location (coordinates), language and number of tweets returned per day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list_date = list()\n",
    "tweets_list_text = list()\n",
    "\n",
    "for date_ in date_list:\n",
    "    text_query = \"Raptors\"\n",
    "    coordinates = '43.651070,-79.347015,50mi'\n",
    "    language = 'en'\n",
    "    result_type = 'recent'\n",
    "    since_date = date_\n",
    "    until_date = (date_ + timedelta(days=1))\n",
    "    max_tweets = 1000\n",
    "    \n",
    "    # Creation of query method using parameters\n",
    "    tweets = tweepy.Cursor(api.search,\n",
    "                           q = text_query,\n",
    "                           geocode = coordinates,\n",
    "                           lang=language,\n",
    "                           result_type = result_type,\n",
    "                           since = since_date,\n",
    "                           until = until_date,\n",
    "                           count = 100).items(max_tweets)\n",
    "    \n",
    "    # List comprehension pulling chosen tweet information from tweets iterable object\n",
    "    for tweet in tweets:\n",
    "        tweets_list_date.append(tweet.created_at)\n",
    "        tweets_list_text.append(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of dataframe from tweets_list\n",
    "tweets_df = pd.DataFrame({'date' : tweets_list_date,'text' : tweets_list_text},columns=['date','text'])\n",
    "# Add an independent column date\n",
    "date = tweets_df['date']\n",
    "date = pd.to_datetime(date).dt.date\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature engineering process is more or less similar to the process for the train dataset, including:\n",
    "- new date-time columns\n",
    "- hashtags and their counts\n",
    "- mentions and their counts\n",
    "- url and their counts\n",
    "\n",
    "The biggest difference, would be not removing the emojis from the text column (yet) for the emoji visualization later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hours\n",
    "tweets_df['hour'] = [dt.hour for dt in tweets_df['date'].astype(object)]\n",
    "# Days\n",
    "tweets_df['day'] = [dt.day for dt in tweets_df['date'].astype(object)]\n",
    "# Month\n",
    "tweets_df['month'] = [dt.month for dt in tweets_df['date'].astype(object)]\n",
    "# Weekday\n",
    "tweets_df['dayofweek'] = [dt.dayofweek for dt in tweets_df['date'].astype(object)]\n",
    "# Delete date column\n",
    "tweets_df = tweets_df.drop(['date'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the hashtags and add the column to the dataset\n",
    "hashtags = []\n",
    "for tweet in tweets_df['text']:\n",
    "    hashtags.append([i  for i in tweet.split() if i.startswith(\"#\") ])\n",
    "tweets_df['hashtags'] = hashtags\n",
    "# Find number of hashtags in each tweet\n",
    "hashtag_counts = []\n",
    "for hashtag in hashtags:\n",
    "    hashtag_counts.append(len(hashtag))\n",
    "tweets_df['hashtag_counts'] = hashtag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the user names and add the column to the dataset\n",
    "users = []\n",
    "for tweet in tweets_df['text']:\n",
    "    users.append([i for i in tweet.split() if i.startswith(\"@\") ])\n",
    "tweets_df['users'] = users\n",
    "# Find number of tagged users in each tweet\n",
    "user_counts = []\n",
    "for user in users:\n",
    "    user_counts.append(len(user))\n",
    "tweets_df['user_counts'] = user_counts\n",
    "# Drop users column\n",
    "tweets_df = tweets_df.drop(['users'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the URLs from the tweets\n",
    "from urlextract import URLExtract\n",
    "extractor = URLExtract()\n",
    "urls = []\n",
    "for i in range(len(tweets_df)):\n",
    "    urls.append(extractor.find_urls(tweets_df['text'][i]))\n",
    "tweets_df['urls'] = urls\n",
    "# Find number of urls in each tweet\n",
    "url_counts = []\n",
    "for url in tweets_df['urls']:\n",
    "    url_counts.append(len(url))\n",
    "tweets_df['url_counts'] = url_counts\n",
    "# Drop urls column\n",
    "tweets_df = tweets_df.drop(['urls'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove excessive information from text Column\n",
    "import re\n",
    "# Creating a function called clean, that removes all hyperlink, hashtags and mentions\n",
    "def clean(x):\n",
    "    x = re.sub(r\"^RT[\\s]+\", \"\", x)\n",
    "    x = re.sub(r\"https?:\\/\\/.*[\\r\\n]*\", \"\", x)\n",
    "    #x = re.sub('[^ ]+\\.[^ ]+','',x)\n",
    "    x = re.sub(r\"#\",\"\", x)\n",
    "    x = re.sub(r\"@[A-Za-z0â€“9]+\",\"\", x)\n",
    "\n",
    "    return x  \n",
    "# Apply the clean function to text column\n",
    "tweets_df['text'] = tweets_df['text'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorization process is the same as well, except we are specifying the vocabulary of the vectorizer from the `reduced_feature.pkl` saved from the earlier notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Load features from training dataset\n",
    "transformer = TfidfTransformer()\n",
    "loaded_features = pickle.load(open(\"pickle/reduced_feature.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Vectorize the text column\n",
    "X_text = tweets_df['text'].astype(str)\n",
    "tfidfconverter = TfidfVectorizer(max_features=10000, \n",
    "                                 min_df=0.0001, max_df=0.5, \n",
    "                                 stop_words=stopwords.words('english'),\n",
    "                                 token_pattern=r'\\b[^\\d\\W]+\\b',\n",
    "                                 strip_accents = \"ascii\",\n",
    "                                 vocabulary = loaded_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the features in test set to train set\n",
    "X_text = transformer.fit_transform(tfidfconverter.fit_transform(X_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text = pd.DataFrame(columns=tfidfconverter.get_feature_names(),data=X_text.toarray())\n",
    "# Retrieve the numerical columns\n",
    "X_num = tweets_df.drop(['text','hashtags'],axis=1)\n",
    "# Concatenate the test dataset\n",
    "X_sample = pd.concat([X_num,X_text],axis=1).astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction & Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing and vectorization, we can pass our new dataset through the trained models, in this case, I have selected the logistic model trained with reduced dataset, as so far, comparing to the model trained with incremental learning, it appears to perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis\n",
    "\n",
    "This is the sentimental analysis for the topic of Raptors over the last 7 days, where the sentiment score falls on the range of -1 to 1, with -1 being the most negative and 1 being the most positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "filename = 'pickle/svc_reduced_model.sav'\n",
    "svc_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sample = svc_model.predict(X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = pd.DataFrame(y_sample,columns = [\"prediction\"])\n",
    "y_prediction = pd.concat([date,y_prediction],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction_table = y_prediction.groupby('date').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction_table.to_csv('howistwitterfeeling/src/streamlit/covid_demo.csv')\n",
    "tweets_df.to_csv('howistwitterfeeling/src/streamlit/covid_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and plot space\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "# Add x-axis and y-axis\n",
    "ax.plot(y_prediction_table.index,\n",
    "           y_prediction_table.prediction,\n",
    "       color='purple')\n",
    "# Set title and labels for axes\n",
    "ax.set(xlabel=\"Date\",\n",
    "       ylabel=\"\",)\n",
    "\n",
    "# Set limit for y scale\n",
    "ax.set_ylim(y_prediction_table.prediction.min()-0.01,y_prediction_table.prediction.max()+0.03)\n",
    "\n",
    "# Define the date format\n",
    "date_form = DateFormatter(\"%m-%d\")\n",
    "ax.xaxis.set_major_formatter(date_form)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 Emojis Associated with Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import regex\n",
    "\n",
    "# Create a function, where emojis found in the texts associated with the topic are returned\n",
    "def split_count(text):\n",
    "\n",
    "    '''\n",
    "    Return the emojis found in the twitter texts associated with the topic\n",
    "    Input: text column from dataset\n",
    "    Output: A list of emojis found in each rows\n",
    "    '''\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "\n",
    "    return emoji_list\n",
    "\n",
    "# Return the emojis found in the twitter texts\n",
    "emoji_rows = tweets_df['text'].apply(split_count)\n",
    "\n",
    "# Return a flattened list of emojis from the topic\n",
    "emoji_list = []\n",
    "for sublist in emoji_rows:\n",
    "    for item in sublist:\n",
    "        emoji_list.append(item)\n",
    "        \n",
    "emoji_count = [[x,emoji_list.count(x)] for x in set(emoji_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list into a dataframe\n",
    "emoji_df = pd.DataFrame(emoji_count,columns=['Emoji','Count']).sort_values('Count',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top 10 emojis\n",
    "emoji_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashtag Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a flattened list of emojis from the topic\n",
    "hashtag_list = []\n",
    "for sublist in tweets_df['hashtags']:\n",
    "    for item in sublist:\n",
    "        hashtag_list.append(item)\n",
    "hashtag_list\n",
    "\n",
    "hashtag_str = ' '.join(hashtag for hashtag in hashtag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pPlot\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud(collocations=False).generate(hashtag_str)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter",
   "language": "python",
   "name": "twitter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
