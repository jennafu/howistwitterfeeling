{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these information found in the EDA, I have engineered a few additional date-time and numerical features in addition to the text data, including:\n",
    "1. Create the hour,day, month,weekday columns from the date column.\n",
    "2. Extract the hashtags and hashtag counts from the text column\n",
    "3. Extract the mentions and mention counts from the text column\n",
    "4. Extract the tagged URL and URL counts from the text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the raw data\n",
    "train = pd.read_csv('data/training.1600000.processed.noemoticon.csv',\n",
    "                       encoding = \"ISO-8859-1\", engine='python',header=None,\n",
    "                      names=['sentiment','tweet_id','date','flag','user','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion and removal of rows and columns explained in EDA\n",
    "# Convert binary classification from {0,4} to {0,1}\n",
    "def convert_sent(x):\n",
    "    if x == 4:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "train['sentiment'] = train['sentiment'].apply(convert_sent)\n",
    "# Find row indexes where the tweed id has exactly one entry\n",
    "id_count = train['tweet_id'].value_counts()\n",
    "id_count[id_count.values == 1].index\n",
    "train = train[train['tweet_id'].isin(id_count[id_count.values == 1].index)].reset_index(drop=True)\n",
    "# Drop date column\n",
    "train = train.drop(['date'],axis=1)\n",
    "# Drop flag column\n",
    "train = train.drop(['flag'],axis=1)\n",
    "# Remove texts appearing more than 20 times\n",
    "train = train[train.groupby('text')['text'].transform('count') < 20].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1592831, 4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datset shape\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the `text` column, extract:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - hashtags\n",
    "  - hashtag counts\n",
    "  - tagged users\n",
    "  - number of tagged users\n",
    "  - websites\n",
    "  - number of websites\n",
    "\n",
    "Then, remove these extra informations from the main text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hashtags & Hashtag Counts***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the hashtags and add the column to the dataset\n",
    "hashtags = []\n",
    "for tweet in train['text']:\n",
    "    hashtags.append([i  for i in tweet.split() if i.startswith(\"#\") ])\n",
    "\n",
    "train['hashtags'] = hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Hashtag Counts:44109\n",
      "Unique Hashtag Counts:15293\n"
     ]
    }
   ],
   "source": [
    "# Find the total and unique number of hashtags\n",
    "hashtags_flat = []\n",
    "for sublist in hashtags:\n",
    "    for item in sublist:\n",
    "        hashtags_flat.append(item)\n",
    "\n",
    "print(f'Total Hashtag Counts:{len(hashtags_flat)}') \n",
    "print(f'Unique Hashtag Counts:{len(set(hashtags_flat))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of hashtags in each tweet\n",
    "hashtag_counts = []\n",
    "for hashtag in hashtags:\n",
    "    hashtag_counts.append(len(hashtag))\n",
    "\n",
    "train['hashtag_counts'] = hashtag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hashtags column\n",
    "train.drop(['hashtags'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tagged Users & Counts***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the user names and add the column to the dataset\n",
    "users = []\n",
    "for tweet in train['text']:\n",
    "    users.append([i for i in tweet.split() if i.startswith(\"@\") ])\n",
    "\n",
    "train['users'] = users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total User Counts:793951\n",
      "Unique User Counts:364032\n"
     ]
    }
   ],
   "source": [
    "# Find the total and unique number of users\n",
    "users_flat = []\n",
    "for sublist in users:\n",
    "    for item in sublist:\n",
    "        users_flat.append(item)\n",
    "\n",
    "print(f'Total User Counts:{len(users_flat)}') \n",
    "print(f'Unique User Counts:{len(set(users_flat))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of tagged users in each tweet\n",
    "user_counts = []\n",
    "for user in users:\n",
    "    user_counts.append(len(user))\n",
    "\n",
    "train['user_counts'] = user_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['user'],axis=1,inplace=True)\n",
    "train.drop(['users'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>flag</th>\n",
       "      <th>text</th>\n",
       "      <th>date_hour</th>\n",
       "      <th>date_day</th>\n",
       "      <th>date_month</th>\n",
       "      <th>date_weekday</th>\n",
       "      <th>hashtag_counts</th>\n",
       "      <th>user_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment    tweet_id      flag  \\\n",
       "0          0  1467810369  NO_QUERY   \n",
       "1          0  1467810672  NO_QUERY   \n",
       "2          0  1467810917  NO_QUERY   \n",
       "3          0  1467811184  NO_QUERY   \n",
       "4          0  1467811193  NO_QUERY   \n",
       "\n",
       "                                                text date_hour  date_day  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...        22         6   \n",
       "1  is upset that he can't update his Facebook by ...        22         6   \n",
       "2  @Kenichan I dived many times for the ball. Man...        22         6   \n",
       "3    my whole body feels itchy and like its on fire         22         6   \n",
       "4  @nationwideclass no, it's not behaving at all....        22         6   \n",
       "\n",
       "   date_month  date_weekday  hashtag_counts  user_counts  \n",
       "0           4             1               0            1  \n",
       "1           4             1               0            0  \n",
       "2           4             1               0            1  \n",
       "3           4             1               0            0  \n",
       "4           4             1               0            1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tagged Websites & Counts***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve urls from tweets using URLExtract\n",
    "# from urlextract import URLExtract\n",
    "\n",
    "# extractor = URLExtract()\n",
    "#urls = []\n",
    "\n",
    "# for i in range(len(train)):\n",
    "    #urls.append(extractor.find_urls(train['text'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save partial dataset\n",
    "# train.to_csv(r'/Users/JennaFu/Desktop/DataScience/BrainStation/Capstone/data/partial_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/partial_data.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove '][' from the `urls` column\n",
    "train['urls'] = train['urls'].str[1:-1]\n",
    "# Split the urls by ','\n",
    "train['urls'] = train['urls'].str.split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of urls in each tweet\n",
    "url_counts = []\n",
    "for url in train['urls']:\n",
    "    if url[0] == '':\n",
    "        url_counts.append(0)\n",
    "    else:\n",
    "        url_counts.append(len(url))\n",
    "train['url_counts'] = url_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Remove excessive information from `text` Column***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Creating a function called clean, that removes all hyperlink, hashtags, mentions and emojis\n",
    "def clean(x):\n",
    "    x = re.sub(r\"^RT[\\s]+\", \"\", x)\n",
    "    x = re.sub(r\"https?:\\/\\/.*[\\r\\n]*\", \"\", x)\n",
    "    x = re.sub('[^ ]+\\.[^ ]+','',x)\n",
    "    x = re.sub(r\"#\",\"\", x)\n",
    "    x = re.sub(r\"@[A-Za-z0–9]+\",\"\", x)\n",
    "    return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the clean function to text column\n",
    "train['text'] = train['text'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the url, user columns from dataset and remove hastag symbols from hashtag column\n",
    "#train.drop(['hashtags'],axis=1,inplace=True)\n",
    "train.drop(['user'],axis=1,inplace=True)\n",
    "#train.drop(['users'],axis=1,inplace=True)\n",
    "#train.drop(['urls'],axis=1,inplace=True)\n",
    "train.drop(['tweet_id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv(r'/Users/JennaFu/Desktop/DataScience/BrainStation/Capstone/data/feature_engineered_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Preprocess the `text` column***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before vectorizing, I have to pre-process the `text` column:\n",
    "\n",
    "- Converting all letters to lower case.\n",
    "- Turning the tweets into tokens. Tokens are words separated by spaces in a text.\n",
    "- Eliminating unwanted characters, such as punctuation marks, special characters, white spaces etc.\n",
    "- Remove stop words, defined by the nltk library.\n",
    "- Apply lemmatization, and returns a word to its base or dictionary form. Example: Better -> Good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have chosen to use the POS tags, as by default, the lemmatizer takes in an input string and tries to lemmatize it, so if you pass in a word, it would lemmatize it treating it as a noun. Hence, To make the lemmatization better and context dependent, we would need to find out the POS tag and pass it on to the lemmatizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize with POS Tag\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function preprocess_tweet_text\n",
    "def preprocess_tweet_text(tweet):\n",
    "    # Convert all characters to lower case\n",
    "    tweet.lower()\n",
    "    # Remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stopwords\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    filtered_words = [w for w in tweet_tokens if not w in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in filtered_words])\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocess_tweet_text to text column\n",
    "train['text'] = train['text'].apply(preprocess_tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed dataset\n",
    "# train.to_csv(r'/Users/JennaFu/Desktop/DataScience/BrainStation/howistwitterfeeling/jupyter notebook/new_data/preprocess_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter",
   "language": "python",
   "name": "twitter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
