{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these information found in the EDA, I have engineered a few additional date-time and numerical features in addition to the text data, including:\n",
    "1. Create the hour,day, month,weekday columns from the date column.\n",
    "2. Extract the hashtags and hashtag counts from the text column\n",
    "3. Extract the mentions and mention counts from the text column\n",
    "4. Extract the tagged URL and URL counts from the text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the raw data\n",
    "train = pd.read_csv('data/training.1600000.processed.noemoticon.csv',\n",
    "                       encoding = \"ISO-8859-1\", engine='python',header=None,\n",
    "                      names=['sentiment','tweet_id','date','flag','user','text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split `date` column into `hour`,`day`, `month`,`weekday` columns, using `train['date'].str[x:y]`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hours\n",
    "train['date_hour'] = train['date'].str[11:13]\n",
    "# Days\n",
    "train['date_day'] = train['date'].str[8:10].astype(int)\n",
    "# Month\n",
    "train['date_month'] = train['date'].str[4:7]\n",
    "# Weekday\n",
    "train['date_weekday'] = train['date'].str[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert month to numerical values\n",
    "train['date_month'].replace('Apr', 4, inplace=True)\n",
    "train['date_month'].replace('May', 5, inplace=True)\n",
    "train['date_month'].replace('Jun', 6, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert weekday to numerical values\n",
    "train['date_weekday'].replace('Mon ', 1, inplace=True)\n",
    "train['date_weekday'].replace('Tue ', 2, inplace=True)\n",
    "train['date_weekday'].replace('Wed ', 3, inplace=True)\n",
    "train['date_weekday'].replace('Thu ', 4, inplace=True)\n",
    "train['date_weekday'].replace('Fri ', 5, inplace=True)\n",
    "train['date_weekday'].replace('Sat ', 6, inplace=True)\n",
    "train['date_weekday'].replace('Sun ', 7, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the date column\n",
    "train.drop(['date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the `text` column, extract:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - hashtags\n",
    "  - hashtag counts\n",
    "  - tagged users\n",
    "  - number of tagged users\n",
    "  - websites\n",
    "  - number of websites\n",
    "\n",
    "Then, remove these extra informations from the main text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hashtags & Hashtag Counts***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the hashtags and add the column to the dataset\n",
    "hashtags = []\n",
    "for tweet in train['text']:\n",
    "    hashtags.append([i  for i in tweet.split() if i.startswith(\"#\") ])\n",
    "\n",
    "train['hashtags'] = hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the total and unique number of hashtags\n",
    "hashtags_flat = []\n",
    "for sublist in hashtags:\n",
    "    for item in sublist:\n",
    "        hashtags_flat.append(item)\n",
    "\n",
    "print(f'Total Hashtag Counts:{len(hashtags_flat)}') \n",
    "print(f'Unique Hashtag Counts:{len(set(hashtags_flat))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of hashtags in each tweet\n",
    "hashtag_counts = []\n",
    "for hashtag in hashtags:\n",
    "    hashtag_counts.append(len(hashtag))\n",
    "\n",
    "train['hashtag_counts'] = hashtag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hashtags column\n",
    "train.drop(['hashtags'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tagged Users & Counts***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the user names and add the column to the dataset\n",
    "users = []\n",
    "for tweet in train['text']:\n",
    "    users.append([i for i in tweet.split() if i.startswith(\"@\") ])\n",
    "\n",
    "train['users'] = users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the total and unique number of users\n",
    "users_flat = []\n",
    "for sublist in users:\n",
    "    for item in sublist:\n",
    "        users_flat.append(item)\n",
    "\n",
    "print(f'Total User Counts:{len(users_flat)}') \n",
    "print(f'Unique User Counts:{len(set(users_flat))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of tagged users in each tweet\n",
    "user_counts = []\n",
    "for user in users:\n",
    "    user_counts.append(len(user))\n",
    "\n",
    "train['user_counts'] = user_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['user'],axis=1,inplace=True)\n",
    "train.drop(['users'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tagged Websites & Counts***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve urls from tweets using URLExtract\n",
    "# from urlextract import URLExtract\n",
    "\n",
    "# extractor = URLExtract()\n",
    "#urls = []\n",
    "\n",
    "# for i in range(len(train)):\n",
    "    #urls.append(extractor.find_urls(train['text'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save partial dataset\n",
    "# train.to_csv(r'/Users/JennaFu/Desktop/DataScience/BrainStation/Capstone/data/partial_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/partial_data.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove '][' from the `urls` column\n",
    "train['urls'] = train['urls'].str[1:-1]\n",
    "# Split the urls by ','\n",
    "train['urls'] = train['urls'].str.split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of urls in each tweet\n",
    "url_counts = []\n",
    "for url in train['urls']:\n",
    "    if url[0] == '':\n",
    "        url_counts.append(0)\n",
    "    else:\n",
    "        url_counts.append(len(url))\n",
    "train['url_counts'] = url_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Remove excessive information from `text` Column***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Creating a function called clean, that removes all hyperlink, hashtags, mentions and emojis\n",
    "def clean(x):\n",
    "    x = re.sub(r\"^RT[\\s]+\", \"\", x)\n",
    "    x = re.sub(r\"https?:\\/\\/.*[\\r\\n]*\", \"\", x)\n",
    "    x = re.sub('[^ ]+\\.[^ ]+','',x)\n",
    "    x = re.sub(r\"#\",\"\", x)\n",
    "    x = re.sub(r\"@[A-Za-z0â€“9]+\",\"\", x)\n",
    "    return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the clean function to text column\n",
    "train['text'] = train['text'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the url, user columns from dataset and remove hastag symbols from hashtag column\n",
    "train.drop(['hashtags'],axis=1,inplace=True)\n",
    "train.drop(['user'],axis=1,inplace=True)\n",
    "train.drop(['users'],axis=1,inplace=True)\n",
    "train.drop(['urls'],axis=1,inplace=True)\n",
    "train.drop(['tweet_id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv(r'/Users/JennaFu/Desktop/DataScience/BrainStation/Capstone/data/feature_engineered_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Preprocess the `text` column***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before vectorizing, I have to pre-process the `text` column:\n",
    "\n",
    "- Converting all letters to lower case.\n",
    "- Turning the tweets into tokens. Tokens are words separated by spaces in a text.\n",
    "- Eliminating unwanted characters, such as punctuation marks, special characters, white spaces etc.\n",
    "- Remove stop words, defined by the nltk library.\n",
    "- Apply lemmatization, and returns a word to its base or dictionary form. Example: Better -> Good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have chosen to use the POS tags, as by default, the lemmatizer takes in an input string and tries to lemmatize it, so if you pass in a word, it would lemmatize it treating it as a noun. Hence, To make the lemmatization better and context dependent, we would need to find out the POS tag and pass it on to the lemmatizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize with POS Tag\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function preprocess_tweet_text\n",
    "def preprocess_tweet_text(tweet):\n",
    "    # Convert all characters to lower case\n",
    "    tweet.lower()\n",
    "    # Remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stopwords\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    filtered_words = [w for w in tweet_tokens if not w in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in filtered_words])\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocess_tweet_text to text column\n",
    "train['text'] = train['text'].apply(preprocess_tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed dataset\n",
    "train.to_csv(r'/Users/JennaFu/Desktop/DataScience/BrainStation/Capstone/data/preprocess_data.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter",
   "language": "python",
   "name": "twitter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
