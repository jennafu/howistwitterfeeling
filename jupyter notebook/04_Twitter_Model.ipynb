{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with Reduced Dataset\n",
    "\n",
    "With the reduced dataset, there are four models I want to attempt to train my model on:\n",
    "- LogisticRegression\n",
    "- LinearSVC\n",
    "- DecisionTreeClassifier\n",
    "- KNeighborsClassifier\n",
    "\n",
    "These are the steps I will be taking in this notebook:\n",
    "1. Compare the baseline models for each classifiers but with their respective best performing scalers.\n",
    "2. Out of the four baseline models, pick the best two and conduct hyperparameter tuning on them.\n",
    "3. Pick the best performing classifier and its respective hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "# Classifiers used\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "from sklearn.externals.joblib import Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the reduced datasets\n",
    "X_train = pd.read_csv('data/reduced_model/X_train_reduced.csv',index_col=0)\n",
    "X_test = pd.read_csv('data/reduced_model/X_test_reduced.csv',index_col=0)\n",
    "y_train = pd.read_csv('data/reduced_model/y_train.csv',index_col=0)\n",
    "y_test = pd.read_csv('data/reduced_model/y_test.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be comparing the baseline models for each classifiers but with their respective best performing scalers. Given that my laptop will have more computing power, I will definitely implement a GridSearchCV to find the best combinations of scalers, classifiers and their hyperparameter. However, given the current restriction, I will be identifying the best choices in each steps manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the train and test sets, with three types of scalers\n",
    "for scaler in [StandardScaler(), MinMaxScaler(), RobustScaler()]:\n",
    "    \n",
    "    # Fit the data to scaler\n",
    "    scaler.fit_transform(X_train)\n",
    "    scaler.transform(X_test)\n",
    "    \n",
    "    # Instantiate and fit to the train set\n",
    "    Logistic = LogisticRegression()\n",
    "    \n",
    "    # Fit the data\n",
    "    Logistic.fit(X_train,y_train)\n",
    "    \n",
    "    # Score the model\n",
    "    print(scaler)\n",
    "    print(Logistic.score(X_train,y_train))\n",
    "    print(Logistic.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to report model accuracy, including train, test and cross validation train accuracy\n",
    "accuracy_data = {'Training accuracy':  [Logistic.score(X_train,y_train)],\n",
    "        'Test accuracy': [Logistic.score(X_test,y_test)],\n",
    "        'Cross validation training accuracy': [np.mean(cross_val_score(Logistic,X_train,y_train))]}\n",
    "\n",
    "accuracy = pd.DataFrame(accuracy_data, \n",
    "                        columns = ['Training accuracy','Test accuracy',\n",
    "                                   'Cross validation training accuracy'],\n",
    "                        index = ['logistic w/ robust scaler']\n",
    "                       )\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the train and test sets, with three types of scalers\n",
    "for scaler in [StandardScaler(), MinMaxScaler(), RobustScaler()]:\n",
    "    \n",
    "    # Fit the data to scaler\n",
    "    scaler.fit_transform(X_train)\n",
    "    scaler.transform(X_test)\n",
    "    \n",
    "    # Instantiate and fit to the train set\n",
    "    SVC = LinearSVC()\n",
    "    \n",
    "    # Fit the data\n",
    "    SVC.fit(X_train,y_train)\n",
    "    \n",
    "    # Score the model\n",
    "    print(scaler)\n",
    "    print(SVC.score(X_train,y_train))\n",
    "    print(SVC.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the RobustScaler is the best scaler to use along the LinearSVC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the accuracy scores of this model to our accuracy score dataframe `accuracy`\n",
    "accuracy_data = {'Training accuracy':  [SVC.score(X_train,y_train)],\n",
    "        'Test accuracy': [SVC.score(X_test,y_test)],\n",
    "        'Cross validation training accuracy': [np.mean(cross_val_score(SVC,X_train,y_train))]\n",
    "        }\n",
    "\n",
    "new_accuracy = pd.DataFrame(accuracy_data, index = [\"SVC w/ robust scaler\"])\n",
    "accuracy = pd.concat([accuracy,new_accuracy])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the train and test sets, with three types of scalers\n",
    "for scaler in [StandardScaler(), MinMaxScaler(), RobustScaler()]:\n",
    "    \n",
    "    # Fit the data to scaler\n",
    "    scaler.fit_transform(X_train)\n",
    "    scaler.transform(X_test)\n",
    "    \n",
    "    # Instantiate and fit to the train set\n",
    "    DT = DecisionTreeClassifier()\n",
    "    \n",
    "    # Fit the data\n",
    "    DT.fit(X_train,y_train)\n",
    "    \n",
    "    # Score the model\n",
    "    print(scaler)\n",
    "    print(DT.score(X_train,y_train))\n",
    "    print(DT.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the accuracy scores of this model to our accuracy score dataframe `accuracy`\n",
    "accuracy_data = {'Training accuracy':  [DT.score(X_train,y_train)],\n",
    "        'Test accuracy': [DT.score(X_test,y_test)],\n",
    "        'Cross validation training accuracy': [np.mean(cross_val_score(DT,X_train,y_train))]\n",
    "        }\n",
    "\n",
    "new_accuracy = pd.DataFrame(accuracy_data, index = [\"DT w/ robust scaler\"])\n",
    "accuracy = pd.concat([accuracy,new_accuracy])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the train and test sets, with three types of scalers\n",
    "for scaler in [StandardScaler(), MinMaxScaler(), RobustScaler()]:\n",
    "    \n",
    "    # Fit the data to scaler\n",
    "    scaler.fit_transform(X_train)\n",
    "    scaler.transform(X_test)\n",
    "    \n",
    "    # Instantiate and fit to the train set\n",
    "    KNN = KNeighborsClassifier()\n",
    "    \n",
    "    # Fit the data\n",
    "    KNN.fit(X_train,y_train)\n",
    "    \n",
    "    # Score the model\n",
    "    print(scaler)\n",
    "    print(KNN.score(X_train,y_train))\n",
    "    print(KNN.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamter Tuning for SVC and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hyperparameter tuning, I will be further splitting my train data into train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train1 = X_train1.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "y_train1 = y_train1.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression\n",
    "Two hyperparamters I will be looking into for LogisticRegression, are its penalty and C. Tuning penalty allows us to compare two regularization techniques L1(Lasso Regression) and L2(Ridge Regression), which introduces different penalty term and works along with the regularization parameter C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Fit the data to scaler\n",
    "scaler = RobustScaler()\n",
    "scaler.fit_transform(X_train1)\n",
    "scaler.transform(X_val)\n",
    "\n",
    "log_comb = []\n",
    "log_train = []\n",
    "log_test = []\n",
    "for i in ['l1','l2']:\n",
    "    for j in [0.0001,0.001,0.01,0.1,1,10]:\n",
    "        clf = LogisticRegression(penalty = i ,C = j,solver ='saga')\n",
    "        clf = clf.fit(X_train1, y_train1)\n",
    "        # Score the model\n",
    "        print(i,j)\n",
    "        log_comb.append([i,j])\n",
    "        log_train.append(clf.score(X_train1,y_train1))   \n",
    "        log_test.append(clf.score(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(0,len(log_comb)),log_train,label='train')\n",
    "plt.plot(range(0,len(log_comb)),log_test,label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the best combination of hyperparameters would be when penalty = l1 and C = 10. Now let's apply them to our train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data to scaler\n",
    "scaler = RobustScaler()\n",
    "scaler.fit_transform(X_train)\n",
    "scaler.transform(X_test)\n",
    "\n",
    "# Instantiate the model\n",
    "clf = LogisticRegression(penalty = 'l1',C = 10,solver='saga')\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# Score the model\n",
    "print(clf.score(X_train,y_train))\n",
    "print(clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC\n",
    "Two hyperparamters I will be looking into for LinearSVC, are its class_weight and C. The reason why class_weight is choosen, is because in the baseline model, I have seen a relatively high false negative rate and low recall rate, leading to less tweets being classified as positive. I have also chosen to tune the regularization parameter C for structural risk minimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Fit the data to scaler\n",
    "scaler = RobustScaler()\n",
    "scaler.fit_transform(X_train1)\n",
    "scaler.transform(X_val)\n",
    "\n",
    "svc_comb = []\n",
    "svc_counter = []\n",
    "svc_score = []\n",
    "for i in [0.0001,0.001,0.01,0.1,1,10]:\n",
    "    for j in range(2,6):\n",
    "        clf = LinearSVC(C = i ,class_weight={-1: 1, 1: j})\n",
    "        clf = clf.fit(X_train1, y_train1)\n",
    "        # Score the model\n",
    "        svc_comb.append([i,j])\n",
    "        svc_counter.append(Counter(clf.predict(X_val)))\n",
    "        svc_score.append(clf.score(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_parameters = {'Combination':  svc_comb,\n",
    "        'Counter': svc_counter,\n",
    "        'Training accuracy': svc_score\n",
    "        }\n",
    "\n",
    "svc_parameters = pd.DataFrame(svc_parameters)\n",
    "svc_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the best combination of hyperparameters might be when C=1(default) and class_weight={-1: 1, 1: 4}, in terms of model accuracy and the classification balance. Now let's apply it to our train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data to scaler\n",
    "scaler = RobustScaler()\n",
    "scaler.fit_transform(X_train)\n",
    "scaler.transform(X_test)\n",
    "\n",
    "# Instantiate the model\n",
    "clf = LinearSVC(C = 1 ,class_weight={-1: 1, 1: 5})\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# Score the model\n",
    "print(clf.score(X_train,y_train))\n",
    "print(clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further investigate whether or not the model was able to resolve the issue with low recall rate, I will be assessing it with the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = clf.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Performing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that out of the two classifiers after hyperparameter tunings, the best performing model is the SVC model with `class_weight = {-1: 1, 1: 5}` and `C = 1`. Hence, we will be using this model to derive sentiments from the real-time Twitter data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'pickle/svc_reduced_model.sav'\n",
    "# save the model to disk\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter",
   "language": "python",
   "name": "twitter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
